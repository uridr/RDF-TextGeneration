# Training optimization
EPOCHS=(2000)
BATCH_SIZE=(256)
MAX_TOKEN=(2048)
PATIENCE=(30)

# Gradient flow
LABEL_SMOOTHING=(0.1)
CLIP_NORM=(0.0)

# Optimizer (Adam) parameters
BETA1=(0.9)
BETA2=(0.98)

# Learning rate and scheduler
LR=(0.044)
LR_SCHEDULER=(inverse_sqrt)

# Warmup the learning rate
WARMUP_UPDATES=(8000)
WARMUP_INIT_LR=(1e-07)

# Dropouts
DROPOUT=(0.1)
ACTIVATION_DROPOUT=(0.1)
ATTENTION_DROPOUT=(0.1)

# Network parameters
ACTIVATION=(relu)
LAYERS=(6)
EMB_DIM=(512)
ENC_POS=(True)
DEC_POS=(True)
FFN_DIM=(2048)
ATT_HEADS=(8)
CROSS=(True)