# Training optimization
EPOCHS=(40)
BATCH_SIZE=(32)

# Gradient flow
LABEL_SMOOTHING=(0.1)
CLIP_NORM=(0.0)

# Optimizer (Adam) parameters
BETA1=(0.9)
BETA2=(0.98)

# Learning rate and scheduler
LR=(1e-03)
LR_SCHEDULER=(inverse_sqrt)

# Warmup the learning rate
WARMUP_UPDATES=(4000)
WARMUP_INIT_LR=(1e-07)

# Network parameters
ACTIVATION=(relu)
DROPOUT=(0.2)
LAYERS=(3)
EMB_DIM=(256)
ENC_POS=(False)
DEC_POS=(False)
FFN_DIM=(1024)
ATT_HEADS=(8)
CROSS=(True)