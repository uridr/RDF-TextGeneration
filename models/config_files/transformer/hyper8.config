# Training optimization
EPOCHS=(200)
BATCH_SIZE=(512)
MAX_TOKEN=(2048)
PATIENCE=(30)

# Gradient flow
LABEL_SMOOTHING=(0.1)
CLIP_NORM=(0.0)

# Optimizer (Adam) parameters
BETA1=(0.9)
BETA2=(0.98)

# Learning rate and scheduler
LR=(1e-03)
LR_SCHEDULER=(inverse_sqrt)

# Warmup the learning rate
WARMUP_UPDATES=(4000)
WARMUP_INIT_LR=(1e-07)

# Dropouts
DROPOUT=(0.1)
ACTIVATION_DROPOUT=(0.2)
ATTENTION_DROPOUT=(0.0)

# Network parameters
ACTIVATION=(relu)
LAYERS=(3)
EMB_DIM=(256)
ENC_POS=(False)
DEC_POS=(False)
FFN_DIM=(1024)
ATT_HEADS=(8)
CROSS=(True)